import pandas as pd

attack_samples = pd.read_csv("https://raw.githubusercontent.com/abiboe/NTmalwaredetection/main/Malware%20dataset.csv")
attack_samples.drop(columns=['Unnamed: 0'],inplace=True)

import numpy as np
unique_api_calls = len(np.unique(attack_samples[:]))
unique_api_calls
import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Input, RNN, SimpleRNN, Dense, Reshape, Dropout
from tensorflow.keras import Model
from tensorflow.keras.optimizers import Adam


class GAN():
    def __init__(self, gan_args):
        [self.batch_size, lr, self.noise_dim, self.data_dim, layers_dim] = gan_args

        self.generator = Generator().\
            build_model(input_shape=(self.noise_dim,), dim=layers_dim, data_dim=self.data_dim)

        self.discriminator = Discriminator(self.batch_size).\
            build_model(input_shape=(self.data_dim,), dim=layers_dim)

        optimizer = tf.keras.optimizers.legacy.Adam(lr, 0.5)

        # Build and compile the discriminator
        self.discriminator.compile(loss='binary_crossentropy',
                                   optimizer=optimizer,
                                   metrics=['accuracy'])

        # The generator takes noise as input and generates data
        z = Input(shape=(self.noise_dim,))
        record = self.generator(z)

        # For the combined model, we will only train the generator
        self.discriminator.trainable = False

        # The discriminator takes generated data as input and determines validity
        validity = self.discriminator(record)

        # The combined model (stacked generator and discriminator)
        # Trains the generator to fool the discriminator
        self.combined = Model(z, validity)
        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)

    def get_data_batch(self, train, batch_size, seed=0):
            # # random sampling - some samples will have excessively low or high sampling, but easy to implement
            # np.random.seed(seed)
            # x = train.loc[ np.random.choice(train.index, batch_size) ].values
            # iterate through shuffled indices, so every sample gets covered evenly

            start_i = (batch_size * seed) % len(train)
            stop_i = start_i + batch_size
            shuffle_seed = (batch_size * seed) // len(train)
            np.random.seed(shuffle_seed)
            train_ix = np.random.choice(list(train.index), replace=False, size=len(train))  # wasteful to shuffle every time
            train_ix = list(train_ix) + list(train_ix)  # duplicate to cover ranges past the end of the set
            x = train.loc[train_ix[start_i: stop_i]].values
            return np.reshape(x, (batch_size, -1))

    def train(self, data, train_arguments):
            [cache_prefix, epochs, sample_interval] = train_arguments

            data_cols = data.columns

            # Adversarial ground truths
            valid = np.ones((self.batch_size, 1))
            fake = np.zeros((self.batch_size, 1))

            for epoch in range(epochs):
                # ---------------------
                #  Train Discriminator
                # ---------------------
                batch_data = self.get_data_batch(data, self.batch_size)
                noise = tf.random.normal((self.batch_size, self.noise_dim))

                # Generate a batch of new data
                gen_data = self.generator.predict(noise)

                # Train the discriminator
                d_loss_real = self.discriminator.train_on_batch(batch_data, valid)
                d_loss_fake = self.discriminator.train_on_batch(gen_data, fake)
                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

                # ---------------------
                #  Train Generator
                # ---------------------
                noise = tf.random.normal((self.batch_size, self.noise_dim))
                # Train the generator (to have the discriminator label samples as valid)
                g_loss = self.combined.train_on_batch(noise, valid)

                # Plot the progress
                print("%d [D loss: %f, acc.: %.2f%%] [G loss: %f]" % (epoch, d_loss[0], 100 * d_loss[1], g_loss))


            # If at save interval => save generated events
            if epoch % sample_interval == 0:
                #Test here data generation step
                # save model checkpoints
                model_checkpoint_base_name = 'model/' + cache_prefix + '_{}_model_weights_step_{}.h5'
                self.generator.save_weights(model_checkpoint_base_name.format('generator', epoch))
                self.discriminator.save_weights(model_checkpoint_base_name.format('discriminator', epoch))

                #Here is generating the data
                z = tf.random.normal((50000, self.noise_dim))
                gen_data = self.generator(z)
                print('generated_attack_samples')

    def save(self, path, name):
        assert os.path.isdir(path) == True, \
            "Please provide a valid path. Path must be a directory."
        model_path = os.path.join(path, name)
        self.generator.save_weights(model_path)  # Load the generator
        return

    def load(self, path):
        assert os.path.isdir(path) == True, \
            "Please provide a valid path. Path must be a directory."
        self.generator = Generator(self.batch_size)
        self.generator = self.generator.load_weights(path)
        return self.generator


class Generator():
    def build_model(self, input_shape, dim, data_dim):
        input = Input(shape=input_shape)
        # Reshape input to include a time step dimension
        x = Reshape((1, -1))(input)
        x = SimpleRNN(dim, activation='LeakyReLU')(x)
        # x = Reshape((-1,))(x)
        x = Dense(data_dim)(x)
        return Model(inputs=input, outputs=x)



class Discriminator():
    def __init__(self, batch_size):
        self.batch_size = batch_size

    def build_model(self, input_shape, dim):
        input = Input(shape=input_shape, batch_size=self.batch_size)
        # Reshape input to include a time step dimension
        x = Reshape((1, -1))(input)
        x = SimpleRNN(dim, activation='LeakyReLU')(x)
        # x = Reshape((-1,))(x)
        x = Dense(1, activation='sigmoid')(x)
        return Model(inputs=input, outputs=x)

# training configuration
noise_dim = 32
dim = 128
batch_size = 128

log_step = 500
epochs =  20000
learning_rate = 0.0005
models_dir = 'model'
gan_args = [batch_size, learning_rate, noise_dim, attack_samples.shape[1], dim]
train_args = ['', epochs, log_step]

from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau

early_stopping = EarlyStopping(monitor="d_loss", mode="min", verbose=1, patience=10)
lr_reduce = ReduceLROnPlateau(monitor='d_loss', factor=0.5, patience=5, mode="min", verbose=1, min_lr=0)

#Training the GAN model
synthesizer = GAN(gan_args)
synthesizer.train(attack_samples, train_args)
import time
time.process_time()
synthesizer.generator.summary()
synthesizer.discriminator.summary()
synthesizer.save('model/gan/saved', 'generator_data')

# Setup parameters visualization parameters
seed = 17
test_size = 50000 # number of fraud cases
noise_dim = 32

np.random.seed(seed)
z = np.random.normal(size=(test_size, noise_dim))
real = synthesizer.get_data_batch(train= attack_samples, batch_size=test_size, seed=seed)
real_samples = pd.DataFrame(real, columns=data_cols)

model_names = ['GAN']
colors = ['deepskyblue','blue']
markers = ['o','^']

#Actual fraud data visualization
model_steps = [ 0, 100, 200, 300, 400, 500, 6000, 10000, 20000, 40000, 50000]
rows = len(model_steps)
columns = 5

base_dir = 'model/'

[model_name, with_class, generator_model] = models['GAN']

# generator_model.load_weights( base_dir + '_generator_model_weights_step_'+str(model_steps)+'.h5')
g_z = generator_model.predict(z)

gen_samples = pd.DataFrame(g_z, columns=data_cols)
gen_samples.to_csv('Generated_Attack_sample-rnnGAN).csv')

from table_evaluator import TableEvaluator

print(len(df), len(gen_samples))
table_evaluator =  TableEvaluator(df, gen_samples)

table_evaluator.visual_evaluation()

import numpy as np
import gym
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

import numpy as np
import gym
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

def create_dqn_model(input_shape, action_space):
    model = Sequential()
    model.add(Dense(512, input_shape=input_shape, activation="relu"))
    model.add(Dense(256, activation="relu"))
    model.add(Dense(64, activation="relu"))
    model.add(Dense(action_space, activation="linear"))
    model.compile(loss="mse", optimizer=Adam())
    return model

model.summary()

import numpy as np
import random
from collections import deque
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

# Parameters
state_size = env.observation_space.shape[0]
action_size = env.action_space.n
batch_size = 64
n_episodes = 10
output_dir = 'model_output/dqn'

# DQN Agent
class DQNAgent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=2000)
        self.gamma = 0.95  # discount rate
        self.epsilon = 1.0  # exploration rate
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.model = self._build_model()

    def _build_model(self):
        model = Sequential()
        model.add(Dense(24, input_dim=self.state_size, activation='relu'))
        model.add(Dense(24, activation='relu'))
        model.add(Dense(self.action_size, activation='linear'))
        model.compile(loss='mse', optimizer=Adam())
        return model

    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state):
        if np.random.rand() <= self.epsilon:
            return random.randrange(self.action_size)
        act_values = self.model.predict(state)
        return np.argmax(act_values[0])

    def replay(self, batch_size):
        minibatch = random.sample(self.memory, batch_size)
        for state, action, reward, next_state, done in minibatch:
            target = self.model.predict(state)
            if done:
                target[0][action] = reward
            else:
                Q_future = max(self.model.predict(next_state)[0])
                target[0][action] = reward + Q_future * self.gamma
            self.model.fit(state, target, epochs=1, verbose=0)
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

# Initialize agent
agent = DQNAgent(state_size, action_size)

# Interact with environment
import matplotlib.pyplot as plt

# Define the 'history' variable before plotting the graphs
history = ...

fig, ax = plt.subplots(1, 2, figsize=[12, 6])
ax[0].plot(history.history["loss"])
ax[0].plot(history.history["val_loss"])
ax[0].set_title("Loss")

ax[1].plot(history.history["accuracy"])
ax[1].plot(history.history["val_accuracy"])
ax[1].legend(("Training", "validation"), loc="lower right")
ax[1].set_title("Accuracy")
ax[1].set_xlabel("Epochs")
ax[0].legend(("Training", "validation"), loc="upper right")
ax[0].set_xlabel("Epochs")
ax[1].plot(history.history["accuracy"])
ax[1].plot(history.history["val_accuracy"])
ax[1].legend(("Training", "validation"), loc="lower right")
ax[1].set_title("Accuracy")
ax[1].set_xlabel("Epochs")

